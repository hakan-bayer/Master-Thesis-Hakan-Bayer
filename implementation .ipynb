{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Here we import the required libraries:\n",
    "#df.to_csv('path',index=False)\n",
    "from IPython.core.display import display, HTML\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "\n",
    "from numpy import arange\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os   \n",
    "import math\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import tree\n",
    "from scipy.stats import sem\n",
    "\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold  \n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor \n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_validate, cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Here we define a score function using stratified cross fold validation.\n",
    "\n",
    "\n",
    "def scores(x):\n",
    "    lin=i()\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True ,random_state=42)\n",
    "    score = cross_val_score(lin, x.iloc[:,2:], x['vb'], cv=cv)\n",
    "    s_min.append(score.min())\n",
    "    s_m.append(score.mean())\n",
    "    s_max.append(score.max())\n",
    "    s_var.append(score.var())\n",
    "\n",
    "    #Checking scores by using our function:\n",
    "    algos=[LinearRegression,KNeighborsRegressor,RandomForestRegressor,Lasso,ElasticNet,DecisionTreeRegressor,GradientBoostingRegressor]\n",
    "    s_min=[]\n",
    "    s_m=[]\n",
    "    s_max=[]\n",
    "    s_var=[]\n",
    "    for i in algos:\n",
    "         scores(i)\n",
    "\n",
    "\n",
    "    models = pd.DataFrame({'Method': ['LinearRegression','KNeighborsRegression','RandomForestRegressor','Lasso','ElasticNet','DecisionTreeRegressor',\n",
    "                                      'GradientBoostingRegressor'],\n",
    "                           'Test_Score_Min':[s_min[0],s_min[1],s_min[2],s_min[3],s_min[4],s_min[5],s_min[6]],\n",
    "                           'Test_Score_Mean':[s_m[0],s_m[1],s_m[2],s_m[3],s_m[4],s_m[5],s_m[6]],\n",
    "                           'Test_Score_Max':[s_max[0],s_max[1],s_max[2],s_max[3],s_max[4],s_max[5],s_max[6]],\n",
    "                           'Test_Score_Var':[s_var[0],s_var[1],s_var[2],s_var[3],s_var[4],s_var[5],s_var[6]]})\n",
    "    models=models.sort_values(by='Test_Score_Mean',ascending=False)\n",
    "    print(list(x.columns))\n",
    "    print(len(list(x.columns)))\n",
    "    return models\n",
    "\n",
    "def y_pred(df,est):\n",
    "\n",
    "    df1=df.copy()\n",
    "    X=df1.iloc[:,1:]\n",
    "    \n",
    "    y=df1['vb']\n",
    "    \n",
    "\n",
    "    stf = StratifiedKFold(n_splits=5,shuffle=True, random_state=42)\n",
    "    \n",
    "    y_pred = cross_val_predict(est, X, y,cv=stf)\n",
    "    score=cross_val_score(est, X, y,cv=stf)\n",
    "    dataset = pd.DataFrame({'vb_real': y, 'vb_prediction': y_pred}, columns=['vb_real', 'vb_prediction'])\n",
    "    results_mean = dataset.groupby(['vb_real'],as_index=False).mean()\n",
    "    results_min = dataset.groupby(['vb_real'],as_index=False).min()\n",
    "    results_max = dataset.groupby(['vb_real'],as_index=False).max()\n",
    "    df1_result=pd.concat([results_mean,results_min,results_max],keys=[' Lasso Mean of bubbles', 'Lasso Min of bubbles','Lasso Max of bubbles'],axis=1)\n",
    "    print(score.mean())\n",
    "\n",
    "    return df1_result\n",
    "\n",
    "def groupby(y,y_pred):\n",
    "    global dataset, dg\n",
    "    dataset = pd.DataFrame({'vb_real': y, 'vb_prediction': y_pred}, columns=['vb_real', 'vb_prediction'])\n",
    "    results_mean = dataset.groupby(['vb_real'],as_index=False).mean()\n",
    "    results_min = dataset.groupby(['vb_real'],as_index=False).min()\n",
    "    results_max = dataset.groupby(['vb_real'],as_index=False).max()\n",
    "    results_std = dataset.groupby(['vb_real'],as_index=False).std()\n",
    "    dg=pd.concat([results_min['vb_prediction'],results_mean['vb_prediction'],results_max['vb_prediction']],keys=['Min', 'Mean', 'Max'],axis=1)\n",
    "    return dg\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def split(df,train_size):       # This is same as StratifiedKFold method takes n% test size from each label of dataset.\n",
    "    global X_train, X_test, y_test, y_train,test_x,train_x\n",
    "    Y=df['vb']\n",
    "    yls=np.array(Y)\n",
    "    yls_val = list(set(yls))\n",
    "    train_x = pd.concat([df[df['vb']==i].sample(frac=train_size, replace=False) for i in yls_val])\n",
    "    X_train = train_x.iloc[:,1:]               #resulting x train\n",
    "    y_train=train_x['vb']\n",
    "    test_x = df.loc[~df.index.isin(train_x.index)]\n",
    "    X_test=test_x.iloc[:,1:]\n",
    "    y_test=test_x['vb']\n",
    "    print(' split(df,train_size) function applies  ShuffledStratifiedKFold cross validation and returns X_test ,X_train, y_test, y_train ')\n",
    "    return     \n",
    "\n",
    "\n",
    "def box_plot(dataset1,dataset2):\n",
    "    dataset1['% deviations']=((dataset1['vb_prediction']-dataset1['vb_real'])/dataset1['vb_real'])*100\n",
    "    dataset2['% deviations']=((dataset2['vb_prediction']-dataset2['vb_real'])/dataset2['vb_real'])*100\n",
    "    import plotly.graph_objects as go\n",
    "\n",
    "    x = dataset1['vb_real']\n",
    "\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Box(\n",
    "        y=dataset1['% deviations'],\n",
    "        x=x,\n",
    "        name='kale',\n",
    "        marker_color='#3D9970'\n",
    "    ))\n",
    "    fig.add_trace(go.Box(\n",
    "        y=dataset2['% deviations'],\n",
    "        x=x,\n",
    "        name='radishes',\n",
    "        marker_color='#FF4136'\n",
    "    ))\n",
    "\n",
    "    fig.update_xaxes(type=\"category\")\n",
    "\n",
    "    fig.update_layout( \n",
    "        xaxis = dict(tickmode = 'auto', dtick = 5, title=\"bubble volume[ÂµL]\"),\n",
    "        yaxis = dict(title=\" relative devition[%]\"),\n",
    "        boxmode='group'  # group together boxes of the different traces for each value of x,\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "    \n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    ''' Root mean squared error regression loss\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : array-like of shape = (n_samples) or (n_samples, n_outputs)\n",
    "    Ground truth (correct) target values.\n",
    "\n",
    "    y_pred : array-like of shape = (n_samples) or (n_samples, n_outputs)\n",
    "    Estimated target values.\n",
    "    '''\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vb</th>\n",
       "      <th>min_all_avg</th>\n",
       "      <th>max_all_avg</th>\n",
       "      <th>mean_all_avg</th>\n",
       "      <th>median_all_avg</th>\n",
       "      <th>time_to_Peak_all_avg</th>\n",
       "      <th>variance_all_avg</th>\n",
       "      <th>rms_all_avg</th>\n",
       "      <th>energy_simpsons_all_avg</th>\n",
       "      <th>crest_factor_all_avg</th>\n",
       "      <th>...</th>\n",
       "      <th>svd_entropy_all_250n_avg</th>\n",
       "      <th>cid_ce_all_250n_avg</th>\n",
       "      <th>app_entropy_all_250n_avg</th>\n",
       "      <th>app_entropy_all_250t_avg</th>\n",
       "      <th>sample_entropy_all_250n_avg</th>\n",
       "      <th>sample_entropy_all_250t_avg</th>\n",
       "      <th>benford_correlation_all_250t_avg</th>\n",
       "      <th>benford_correlation_all_250n_avg</th>\n",
       "      <th>min_all_250n_avg</th>\n",
       "      <th>min_all_250t_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>-8.884786</td>\n",
       "      <td>18.013173</td>\n",
       "      <td>1.121821</td>\n",
       "      <td>0.390724</td>\n",
       "      <td>246.081633</td>\n",
       "      <td>130.926580</td>\n",
       "      <td>5.196627</td>\n",
       "      <td>896.540597</td>\n",
       "      <td>3.429126</td>\n",
       "      <td>...</td>\n",
       "      <td>0.989924</td>\n",
       "      <td>22.430463</td>\n",
       "      <td>0.998827</td>\n",
       "      <td>1.009635</td>\n",
       "      <td>1.700297</td>\n",
       "      <td>1.668619</td>\n",
       "      <td>0.771037</td>\n",
       "      <td>0.795747</td>\n",
       "      <td>-5.599072</td>\n",
       "      <td>-5.456215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>-8.525532</td>\n",
       "      <td>16.188754</td>\n",
       "      <td>0.542835</td>\n",
       "      <td>0.280590</td>\n",
       "      <td>257.183673</td>\n",
       "      <td>38.438620</td>\n",
       "      <td>3.950886</td>\n",
       "      <td>435.254564</td>\n",
       "      <td>3.640311</td>\n",
       "      <td>...</td>\n",
       "      <td>0.993032</td>\n",
       "      <td>23.072155</td>\n",
       "      <td>1.015609</td>\n",
       "      <td>1.019815</td>\n",
       "      <td>1.620138</td>\n",
       "      <td>1.653597</td>\n",
       "      <td>0.785080</td>\n",
       "      <td>0.786250</td>\n",
       "      <td>-5.709206</td>\n",
       "      <td>-5.484716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>-7.014213</td>\n",
       "      <td>10.842930</td>\n",
       "      <td>0.375405</td>\n",
       "      <td>0.210277</td>\n",
       "      <td>237.653061</td>\n",
       "      <td>17.663462</td>\n",
       "      <td>3.177249</td>\n",
       "      <td>299.684796</td>\n",
       "      <td>3.159494</td>\n",
       "      <td>...</td>\n",
       "      <td>0.995424</td>\n",
       "      <td>22.882751</td>\n",
       "      <td>1.013498</td>\n",
       "      <td>1.012926</td>\n",
       "      <td>1.641626</td>\n",
       "      <td>1.657901</td>\n",
       "      <td>0.793707</td>\n",
       "      <td>0.791318</td>\n",
       "      <td>-5.565233</td>\n",
       "      <td>-5.891764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>-6.170402</td>\n",
       "      <td>15.727557</td>\n",
       "      <td>0.771562</td>\n",
       "      <td>0.441843</td>\n",
       "      <td>228.897959</td>\n",
       "      <td>50.928493</td>\n",
       "      <td>3.845624</td>\n",
       "      <td>616.637645</td>\n",
       "      <td>3.035497</td>\n",
       "      <td>...</td>\n",
       "      <td>0.995611</td>\n",
       "      <td>23.379019</td>\n",
       "      <td>1.014707</td>\n",
       "      <td>1.020296</td>\n",
       "      <td>1.619902</td>\n",
       "      <td>1.698297</td>\n",
       "      <td>0.778885</td>\n",
       "      <td>0.775844</td>\n",
       "      <td>-5.313259</td>\n",
       "      <td>-5.333668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>-7.955254</td>\n",
       "      <td>13.861073</td>\n",
       "      <td>0.520818</td>\n",
       "      <td>0.299848</td>\n",
       "      <td>251.306122</td>\n",
       "      <td>26.020647</td>\n",
       "      <td>3.645854</td>\n",
       "      <td>415.628219</td>\n",
       "      <td>3.413758</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992663</td>\n",
       "      <td>22.950719</td>\n",
       "      <td>0.998641</td>\n",
       "      <td>1.011205</td>\n",
       "      <td>1.673978</td>\n",
       "      <td>1.643214</td>\n",
       "      <td>0.806397</td>\n",
       "      <td>0.785757</td>\n",
       "      <td>-5.587907</td>\n",
       "      <td>-5.853213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>900.0</td>\n",
       "      <td>-53.846611</td>\n",
       "      <td>416.541144</td>\n",
       "      <td>53.905302</td>\n",
       "      <td>-0.581305</td>\n",
       "      <td>295.693878</td>\n",
       "      <td>52030.242876</td>\n",
       "      <td>142.085187</td>\n",
       "      <td>43125.751745</td>\n",
       "      <td>3.426954</td>\n",
       "      <td>...</td>\n",
       "      <td>0.931820</td>\n",
       "      <td>19.122217</td>\n",
       "      <td>0.883004</td>\n",
       "      <td>0.797100</td>\n",
       "      <td>1.504239</td>\n",
       "      <td>1.276669</td>\n",
       "      <td>0.765879</td>\n",
       "      <td>0.778891</td>\n",
       "      <td>-8.622121</td>\n",
       "      <td>-16.071101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>900.0</td>\n",
       "      <td>-54.105824</td>\n",
       "      <td>410.649278</td>\n",
       "      <td>52.118998</td>\n",
       "      <td>-0.320110</td>\n",
       "      <td>383.142857</td>\n",
       "      <td>45348.131648</td>\n",
       "      <td>135.524884</td>\n",
       "      <td>41694.792012</td>\n",
       "      <td>3.532173</td>\n",
       "      <td>...</td>\n",
       "      <td>0.928225</td>\n",
       "      <td>19.513433</td>\n",
       "      <td>0.916231</td>\n",
       "      <td>0.839651</td>\n",
       "      <td>1.462180</td>\n",
       "      <td>1.363634</td>\n",
       "      <td>0.810487</td>\n",
       "      <td>0.788090</td>\n",
       "      <td>-7.616028</td>\n",
       "      <td>-15.024191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>900.0</td>\n",
       "      <td>-50.675751</td>\n",
       "      <td>414.956902</td>\n",
       "      <td>51.504785</td>\n",
       "      <td>-0.420649</td>\n",
       "      <td>348.326531</td>\n",
       "      <td>49000.747762</td>\n",
       "      <td>138.199145</td>\n",
       "      <td>41203.622544</td>\n",
       "      <td>3.563863</td>\n",
       "      <td>...</td>\n",
       "      <td>0.910223</td>\n",
       "      <td>19.820555</td>\n",
       "      <td>0.927078</td>\n",
       "      <td>0.912389</td>\n",
       "      <td>1.448568</td>\n",
       "      <td>1.394955</td>\n",
       "      <td>0.800069</td>\n",
       "      <td>0.777357</td>\n",
       "      <td>-9.083914</td>\n",
       "      <td>-10.961465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>900.0</td>\n",
       "      <td>-52.202568</td>\n",
       "      <td>410.021922</td>\n",
       "      <td>52.065595</td>\n",
       "      <td>-0.049507</td>\n",
       "      <td>357.979592</td>\n",
       "      <td>45564.230598</td>\n",
       "      <td>137.627301</td>\n",
       "      <td>41653.816515</td>\n",
       "      <td>3.447565</td>\n",
       "      <td>...</td>\n",
       "      <td>0.906716</td>\n",
       "      <td>19.341111</td>\n",
       "      <td>0.919419</td>\n",
       "      <td>0.882263</td>\n",
       "      <td>1.455675</td>\n",
       "      <td>1.344003</td>\n",
       "      <td>0.771693</td>\n",
       "      <td>0.762539</td>\n",
       "      <td>-8.590323</td>\n",
       "      <td>-12.386241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>900.0</td>\n",
       "      <td>-53.567995</td>\n",
       "      <td>410.003434</td>\n",
       "      <td>49.824990</td>\n",
       "      <td>-0.323097</td>\n",
       "      <td>343.489796</td>\n",
       "      <td>48274.380644</td>\n",
       "      <td>133.424172</td>\n",
       "      <td>39859.539447</td>\n",
       "      <td>3.563571</td>\n",
       "      <td>...</td>\n",
       "      <td>0.919922</td>\n",
       "      <td>20.553942</td>\n",
       "      <td>0.956322</td>\n",
       "      <td>0.876767</td>\n",
       "      <td>1.533548</td>\n",
       "      <td>1.333342</td>\n",
       "      <td>0.771832</td>\n",
       "      <td>0.682245</td>\n",
       "      <td>-7.057791</td>\n",
       "      <td>-13.139423</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>352 rows Ã 79 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        vb  min_all_avg  max_all_avg  mean_all_avg  median_all_avg  \\\n",
       "0      5.0    -8.884786    18.013173      1.121821        0.390724   \n",
       "1      5.0    -8.525532    16.188754      0.542835        0.280590   \n",
       "2      5.0    -7.014213    10.842930      0.375405        0.210277   \n",
       "3      5.0    -6.170402    15.727557      0.771562        0.441843   \n",
       "4      5.0    -7.955254    13.861073      0.520818        0.299848   \n",
       "..     ...          ...          ...           ...             ...   \n",
       "347  900.0   -53.846611   416.541144     53.905302       -0.581305   \n",
       "348  900.0   -54.105824   410.649278     52.118998       -0.320110   \n",
       "349  900.0   -50.675751   414.956902     51.504785       -0.420649   \n",
       "350  900.0   -52.202568   410.021922     52.065595       -0.049507   \n",
       "351  900.0   -53.567995   410.003434     49.824990       -0.323097   \n",
       "\n",
       "     time_to_Peak_all_avg  variance_all_avg  rms_all_avg  \\\n",
       "0              246.081633        130.926580     5.196627   \n",
       "1              257.183673         38.438620     3.950886   \n",
       "2              237.653061         17.663462     3.177249   \n",
       "3              228.897959         50.928493     3.845624   \n",
       "4              251.306122         26.020647     3.645854   \n",
       "..                    ...               ...          ...   \n",
       "347            295.693878      52030.242876   142.085187   \n",
       "348            383.142857      45348.131648   135.524884   \n",
       "349            348.326531      49000.747762   138.199145   \n",
       "350            357.979592      45564.230598   137.627301   \n",
       "351            343.489796      48274.380644   133.424172   \n",
       "\n",
       "     energy_simpsons_all_avg  crest_factor_all_avg  ...  \\\n",
       "0                 896.540597              3.429126  ...   \n",
       "1                 435.254564              3.640311  ...   \n",
       "2                 299.684796              3.159494  ...   \n",
       "3                 616.637645              3.035497  ...   \n",
       "4                 415.628219              3.413758  ...   \n",
       "..                       ...                   ...  ...   \n",
       "347             43125.751745              3.426954  ...   \n",
       "348             41694.792012              3.532173  ...   \n",
       "349             41203.622544              3.563863  ...   \n",
       "350             41653.816515              3.447565  ...   \n",
       "351             39859.539447              3.563571  ...   \n",
       "\n",
       "     svd_entropy_all_250n_avg  cid_ce_all_250n_avg  app_entropy_all_250n_avg  \\\n",
       "0                    0.989924            22.430463                  0.998827   \n",
       "1                    0.993032            23.072155                  1.015609   \n",
       "2                    0.995424            22.882751                  1.013498   \n",
       "3                    0.995611            23.379019                  1.014707   \n",
       "4                    0.992663            22.950719                  0.998641   \n",
       "..                        ...                  ...                       ...   \n",
       "347                  0.931820            19.122217                  0.883004   \n",
       "348                  0.928225            19.513433                  0.916231   \n",
       "349                  0.910223            19.820555                  0.927078   \n",
       "350                  0.906716            19.341111                  0.919419   \n",
       "351                  0.919922            20.553942                  0.956322   \n",
       "\n",
       "     app_entropy_all_250t_avg  sample_entropy_all_250n_avg  \\\n",
       "0                    1.009635                     1.700297   \n",
       "1                    1.019815                     1.620138   \n",
       "2                    1.012926                     1.641626   \n",
       "3                    1.020296                     1.619902   \n",
       "4                    1.011205                     1.673978   \n",
       "..                        ...                          ...   \n",
       "347                  0.797100                     1.504239   \n",
       "348                  0.839651                     1.462180   \n",
       "349                  0.912389                     1.448568   \n",
       "350                  0.882263                     1.455675   \n",
       "351                  0.876767                     1.533548   \n",
       "\n",
       "     sample_entropy_all_250t_avg  benford_correlation_all_250t_avg  \\\n",
       "0                       1.668619                          0.771037   \n",
       "1                       1.653597                          0.785080   \n",
       "2                       1.657901                          0.793707   \n",
       "3                       1.698297                          0.778885   \n",
       "4                       1.643214                          0.806397   \n",
       "..                           ...                               ...   \n",
       "347                     1.276669                          0.765879   \n",
       "348                     1.363634                          0.810487   \n",
       "349                     1.394955                          0.800069   \n",
       "350                     1.344003                          0.771693   \n",
       "351                     1.333342                          0.771832   \n",
       "\n",
       "     benford_correlation_all_250n_avg  min_all_250n_avg  min_all_250t_avg  \n",
       "0                            0.795747         -5.599072         -5.456215  \n",
       "1                            0.786250         -5.709206         -5.484716  \n",
       "2                            0.791318         -5.565233         -5.891764  \n",
       "3                            0.775844         -5.313259         -5.333668  \n",
       "4                            0.785757         -5.587907         -5.853213  \n",
       "..                                ...               ...               ...  \n",
       "347                          0.778891         -8.622121        -16.071101  \n",
       "348                          0.788090         -7.616028        -15.024191  \n",
       "349                          0.777357         -9.083914        -10.961465  \n",
       "350                          0.762539         -8.590323        -12.386241  \n",
       "351                          0.682245         -7.057791        -13.139423  \n",
       "\n",
       "[352 rows x 79 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('all_250t_250n_max_min_std_avg_78_features.csv')\n",
    "df=df.iloc[:,:]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def las1():\n",
    "    global search1, search2, search3, y_te_1, y_te_2,y_te_3, l1,l2,l3\n",
    "    \n",
    "\n",
    "    # loading libraries\n",
    "    from sklearn import decomposition, datasets\n",
    "    from sklearn import linear_model\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    # Creating an scaler object\n",
    "    std_slc = StandardScaler()\n",
    "\n",
    "    # Creating a linear regression object with an L2 penalty\n",
    "    lasso = linear_model.Lasso()\n",
    "\n",
    "    # Creating a pipeline of three steps. First, standardize the data.\n",
    "    # Second, tranform the data with PCA.\n",
    "    # Third, train a Decision Tree Classifier on the data.\n",
    "    pipe = Pipeline(steps=[('std_slc', std_slc),('lasso', lasso)])\n",
    "\n",
    "\n",
    "    # Creating lists of parameter for Lasso Regression\n",
    "    alpha = arange(0,1, 0.001)\n",
    "    # Creating a dictionary of all the parameter options \n",
    "    # Note has you can access the parameters of steps of a pipeline by using '__â\n",
    "    parameters = dict(lasso__alpha=alpha)\n",
    "\n",
    "    # Conducting Parameter Optmization With Pipeline\n",
    "    # Create a grid search object\n",
    "\n",
    "    search1 = GridSearchCV(pipe, parameters,cv=5, n_jobs=-1)\n",
    "    search2 = GridSearchCV(pipe, parameters,cv=5, n_jobs=-1)\n",
    "    search3 = GridSearchCV(pipe, parameters,cv=5, n_jobs=-1)\n",
    "    \n",
    "    \n",
    "def elastic1():\n",
    "    global search4, search5, search6, y_te_4, y_te_5,y_te_6, l4,l5,l6\n",
    "\n",
    "\n",
    "    # loading libraries\n",
    "    from sklearn import decomposition, datasets\n",
    "    from sklearn import linear_model\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    # Creating an scaler object\n",
    "    std_slc = StandardScaler()\n",
    "\n",
    "    # Creating a linear regression object with an L2 penalty\n",
    "    elastic = linear_model.ElasticNet()\n",
    "\n",
    "    # Creating a pipeline of three steps. First, standardize the data.\n",
    "    # Second, tranform the data with PCA.\n",
    "    # Third, train a Decision Tree Classifier on the data.\n",
    "    pipe = Pipeline(steps=[('std_slc', std_slc),('elastic', elastic)])\n",
    "\n",
    "\n",
    "    # Creating lists of parameter for Lasso Regression\n",
    "    alpha = arange(0,1, 0.001)\n",
    "    # Creating a dictionary of all the parameter options \n",
    "    # Note has you can access the parameters of steps of a pipeline by using '__â\n",
    "    parameters = dict(elastic__alpha=alpha)\n",
    "\n",
    "    # Conducting Parameter Optmization With Pipeline\n",
    "    # Create a grid search object\n",
    "\n",
    "    search4 = GridSearchCV(pipe, parameters,cv=5, n_jobs=-1)\n",
    "    search5 = GridSearchCV(pipe, parameters,cv=5, n_jobs=-1)\n",
    "    search6 = GridSearchCV(pipe, parameters,cv=5, n_jobs=-1)    \n",
    "    \n",
    "    \n",
    "def tree():\n",
    "    \n",
    "    global search7, search8, search9, y_te_7, y_te_8,y_te_9, l7,l8,l9\n",
    "   \n",
    "    \n",
    "\n",
    "\n",
    "    # loading libraries\n",
    "    from sklearn import decomposition, datasets\n",
    "    from sklearn import tree\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "    # Creating a DecisionTreeRegressor object\n",
    "    dtreeReg = tree.DecisionTreeRegressor()\n",
    "\n",
    "    # Creating a pipeline of three steps. First, standardize the data .\n",
    "    # Second, tranform the data with PCA.\n",
    "    # Third, train a Decision Tree Classifier on the data.\n",
    "    pipe = Pipeline(steps=[('dtreeReg', dtreeReg)])\n",
    "\n",
    "   \n",
    "\n",
    "    # Creating lists of parameter for DecisionTreeRegressor\n",
    "    max_depth = [3,4,5]\n",
    "    min_samples_leaf = [2,3,4,5,6,7,8,9,10]\n",
    "    \n",
    "\n",
    "    # Creating a dictionary of all the parameter options \n",
    "    # Note has you can access the parameters of steps of a pipeline by using '__â\n",
    "    parameters = dict(dtreeReg__max_depth=max_depth, dtreeReg__min_samples_leaf=min_samples_leaf)\n",
    "\n",
    "    \n",
    "    \n",
    "    #rsf= RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=0)\n",
    "    \n",
    "    # Conducting Parameter Optmization With Pipeline\n",
    "    # Creating a grid search object\n",
    "    search7 = GridSearchCV(pipe, parameters,cv=5, n_jobs=-1)\n",
    "    search8 = GridSearchCV(pipe, parameters,cv=5, n_jobs=-1)\n",
    "    search9 = GridSearchCV(pipe, parameters,cv=5, n_jobs=-1)    \n",
    "    \n",
    "    \n",
    "def forest():\n",
    "    \n",
    "    global search10, search11, search12, y_te_10, y_te_11,y_te_12, l10,l11,l12\n",
    "    \n",
    "    \n",
    "\n",
    "    # loading libraries\n",
    "    from sklearn import decomposition, datasets\n",
    "    from sklearn import tree\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.model_selection import RandomizedSearchCV\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "    # Creating a DecisionTreeRegressor object\n",
    "  \n",
    "    rf = RandomForestRegressor()\n",
    "\n",
    "    # Creating a pipeline of three steps. First, standardize the data .\n",
    "    # Second, tranform the data with PCA.\n",
    "    # Third, train a Decision Tree Classifier on the data.\n",
    "    pipe = Pipeline(steps=[('rf', rf )])\n",
    "\n",
    "    # Creating Parameter Space\n",
    "    # Creating a list of a sequence of integers from 1 to 30 (the number of features in X + 1)\n",
    "   \n",
    "\n",
    "    # Creating lists of parameter for DecisionTreeRegressor\n",
    "    # Maximum number of levels in tree\n",
    "    max_depth = [3,4,5]\n",
    "    min_samples_leaf = list(arange(3,15))\n",
    "    min_samples_split=list(arange(2,11))\n",
    "    # Number of trees in random forest\n",
    "    n_estimators = [100]\n",
    "    \n",
    "    # Creating a dictionary of all the parameter options \n",
    "    # Note has you can access the parameters of steps of a pipeline by using '__â\n",
    "    parameters = dict( rf__max_depth=max_depth, rf__min_samples_leaf=min_samples_leaf, rf__n_estimators=n_estimators,rf__min_samples_split=min_samples_split)\n",
    "\n",
    "    \n",
    "   # Fitting the grid search\n",
    "    \n",
    "    search10 = GridSearchCV(pipe, parameters,cv=5, verbose=2, n_jobs = -1)\n",
    "    search11 = GridSearchCV(pipe, parameters,cv=5,verbose=2, n_jobs = -1)\n",
    "    search12 = GridSearchCV(pipe, parameters,cv=5,verbose=2, n_jobs = -1)    \n",
    "    \n",
    "def xgbr():\n",
    "    \n",
    "    global search13, search14, search15, y_te_13, y_te_14,y_te_15, l13,l14,l15\n",
    " \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn import preprocessing\n",
    "    from xgboost.sklearn import XGBRegressor\n",
    "    import datetime\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "    now = datetime.datetime.now()\n",
    "\n",
    "\n",
    "    # Various hyper-parameters to tune\n",
    "    xgb1 = XGBRegressor()\n",
    "    parameters = { 'learning_rate': [.03, 0.05, .07], #so called `eta` value\n",
    "              'max_depth': [3,4,5],\n",
    "              'min_child_weight': list(arange(1,17)),\n",
    "              'min_split_loss': list(arange(100,520,20)),\n",
    "              'subsample': [0.3,0.4,0.5,0.6,0.7,0.8]}\n",
    "\n",
    "    search13 = GridSearchCV(xgb1,parameters,cv = 5,n_jobs = -1,verbose=True)\n",
    "    search14 = GridSearchCV(xgb1,parameters,cv = 5,n_jobs = -1,verbose=True)\n",
    "    search15 = GridSearchCV(xgb1,parameters,cv = 5,n_jobs = -1,verbose=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " split(df,train_size) function applies  ShuffledStratifiedKFold cross validation and returns X_test ,X_train, y_test, y_train \n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   35.7s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   44.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   40.0s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   51.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   45.5s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   58.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 45 candidates, totalling 225 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   39.4s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 225 out of 225 | elapsed:  3.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:48:55] WARNING: d:\\bld\\xgboost-split_1615294821523\\work\\src\\objective\\regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[19:48:55] WARNING: ..\\src\\learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "Fitting 5 folds for each of 45 candidates, totalling 225 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   50.1s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done 225 out of 225 | elapsed:  4.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:53:52] WARNING: d:\\bld\\xgboost-split_1615294821523\\work\\src\\objective\\regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[19:53:52] WARNING: ..\\src\\learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "Fitting 5 folds for each of 45 candidates, totalling 225 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  5.3min\n",
      "[Parallel(n_jobs=-1)]: Done 225 out of 225 | elapsed:  6.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:00:08] WARNING: d:\\bld\\xgboost-split_1615294821523\\work\\src\\objective\\regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[20:00:08] WARNING: ..\\src\\learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      " split(df,train_size) function applies  ShuffledStratifiedKFold cross validation and returns X_test ,X_train, y_test, y_train \n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   36.3s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   45.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   40.3s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   51.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   45.7s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   58.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 45 candidates, totalling 225 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   39.3s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 225 out of 225 | elapsed:  3.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:38:15] WARNING: d:\\bld\\xgboost-split_1615294821523\\work\\src\\objective\\regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[20:38:15] WARNING: ..\\src\\learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "Fitting 5 folds for each of 45 candidates, totalling 225 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   50.2s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done 225 out of 225 | elapsed:  4.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:43:11] WARNING: d:\\bld\\xgboost-split_1615294821523\\work\\src\\objective\\regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[20:43:11] WARNING: ..\\src\\learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "Fitting 5 folds for each of 45 candidates, totalling 225 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  5.3min\n",
      "[Parallel(n_jobs=-1)]: Done 225 out of 225 | elapsed:  6.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:49:27] WARNING: d:\\bld\\xgboost-split_1615294821523\\work\\src\\objective\\regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[20:49:27] WARNING: ..\\src\\learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      " split(df,train_size) function applies  ShuffledStratifiedKFold cross validation and returns X_test ,X_train, y_test, y_train \n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   35.6s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   44.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   41.6s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   53.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   45.7s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   58.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 45 candidates, totalling 225 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   39.4s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 225 out of 225 | elapsed:  3.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:27:35] WARNING: d:\\bld\\xgboost-split_1615294821523\\work\\src\\objective\\regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[21:27:35] WARNING: ..\\src\\learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "Fitting 5 folds for each of 45 candidates, totalling 225 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   49.8s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=-1)]: Done 225 out of 225 | elapsed:  5.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:32:44] WARNING: d:\\bld\\xgboost-split_1615294821523\\work\\src\\objective\\regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[21:32:44] WARNING: ..\\src\\learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "Fitting 5 folds for each of 45 candidates, totalling 225 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  5.3min\n",
      "[Parallel(n_jobs=-1)]: Done 225 out of 225 | elapsed:  6.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:39:04] WARNING: d:\\bld\\xgboost-split_1615294821523\\work\\src\\objective\\regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[21:39:04] WARNING: ..\\src\\learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      " split(df,train_size) function applies  ShuffledStratifiedKFold cross validation and returns X_test ,X_train, y_test, y_train \n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   36.1s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   44.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   40.7s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   51.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   45.7s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   58.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 45 candidates, totalling 225 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   39.1s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 225 out of 225 | elapsed:  3.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:17:02] WARNING: d:\\bld\\xgboost-split_1615294821523\\work\\src\\objective\\regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:17:02] WARNING: ..\\src\\learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "Fitting 5 folds for each of 45 candidates, totalling 225 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   50.7s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done 225 out of 225 | elapsed:  5.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:22:03] WARNING: d:\\bld\\xgboost-split_1615294821523\\work\\src\\objective\\regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:22:03] WARNING: ..\\src\\learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "Fitting 5 folds for each of 45 candidates, totalling 225 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  5.3min\n",
      "[Parallel(n_jobs=-1)]: Done 225 out of 225 | elapsed:  6.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:28:21] WARNING: d:\\bld\\xgboost-split_1615294821523\\work\\src\\objective\\regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:28:21] WARNING: ..\\src\\learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      " split(df,train_size) function applies  ShuffledStratifiedKFold cross validation and returns X_test ,X_train, y_test, y_train \n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   35.5s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   44.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   39.9s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   50.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   46.8s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   58.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 45 candidates, totalling 225 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   39.1s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 225 out of 225 | elapsed:  3.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:06:23] WARNING: d:\\bld\\xgboost-split_1615294821523\\work\\src\\objective\\regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[23:06:23] WARNING: ..\\src\\learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "Fitting 5 folds for each of 45 candidates, totalling 225 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   49.4s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done 225 out of 225 | elapsed:  5.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:11:24] WARNING: d:\\bld\\xgboost-split_1615294821523\\work\\src\\objective\\regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[23:11:24] WARNING: ..\\src\\learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "Fitting 5 folds for each of 45 candidates, totalling 225 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  5.3min\n",
      "[Parallel(n_jobs=-1)]: Done 225 out of 225 | elapsed:  6.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:17:43] WARNING: d:\\bld\\xgboost-split_1615294821523\\work\\src\\objective\\regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[23:17:43] WARNING: ..\\src\\learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "l1=[]\n",
    "l2=[]\n",
    "l3=[]\n",
    "l4=[]\n",
    "l5=[]\n",
    "l6=[]\n",
    "l7=[]\n",
    "l8=[]\n",
    "l9=[]\n",
    "l10=[]\n",
    "l11=[]\n",
    "l12=[]\n",
    "l13=[]\n",
    "l14=[]\n",
    "l15=[]\n",
    "\n",
    "\n",
    "for i in range(0,5):\n",
    "\n",
    "    split(df,0.8)\n",
    "\n",
    "    \n",
    "    las1()\n",
    "    search1.fit(X_train.iloc[:,:40], y_train)\n",
    "    search2.fit(X_train.iloc[:,:58], y_train)\n",
    "    search3.fit(X_train.iloc[:,:78], y_train)\n",
    "    \n",
    "    elastic1()\n",
    "    search4.fit(X_train.iloc[:,:40], y_train)\n",
    "    search5.fit(X_train.iloc[:,:58], y_train)\n",
    "    search6.fit(X_train.iloc[:,:78], y_train)\n",
    "    \n",
    "    tree()\n",
    "    search7.fit(X_train.iloc[:,:40], y_train)\n",
    "    search8.fit(X_train.iloc[:,:58], y_train)\n",
    "    search9.fit(X_train.iloc[:,:78], y_train)\n",
    "    \n",
    "    forest()\n",
    "    search10.fit(X_train.iloc[:,:40], y_train)\n",
    "    search11.fit(X_train.iloc[:,:58], y_train)\n",
    "    search12.fit(X_train.iloc[:,:78], y_train)\n",
    "    \n",
    "    xgbr()\n",
    "    search13.fit(X_train.iloc[:,:40], y_train)\n",
    "    search14.fit(X_train.iloc[:,:58], y_train)\n",
    "    search15.fit(X_train.iloc[:,:78], y_train)\n",
    "    \n",
    "    \n",
    "    \n",
    "    rmse_te_1=root_mean_squared_error(y_test, search1.predict(X_test.iloc[:,:40]))\n",
    "    rmse_tr_1=root_mean_squared_error(y_train, search1.predict(X_train.iloc[:,:40]))\n",
    "    y_te_1= search1.predict(X_test.iloc[:,:40])\n",
    "    rmse_te_2=root_mean_squared_error(y_test, search2.predict(X_test.iloc[:,:58]))\n",
    "    rmse_tr_2=root_mean_squared_error(y_train, search2.predict(X_train.iloc[:,:58]))\n",
    "    y_te_2= search2.predict(X_test.iloc[:,:58]) \n",
    "    rmse_te_3=root_mean_squared_error(y_test, search3.predict(X_test.iloc[:,:78]))\n",
    "    rmse_tr_3=root_mean_squared_error(y_train, search3.predict(X_train.iloc[:,:78]))\n",
    "    y_te_3= search3.predict(X_test.iloc[:,:78])\n",
    "    \n",
    "    rmse_te_4=root_mean_squared_error(y_test, search4.predict(X_test.iloc[:,:40]))\n",
    "    rmse_tr_4=root_mean_squared_error(y_train, search4.predict(X_train.iloc[:,:40]))\n",
    "    y_te_4= search4.predict(X_test.iloc[:,:40])\n",
    "    rmse_te_5=root_mean_squared_error(y_test, search5.predict(X_test.iloc[:,:58]))\n",
    "    rmse_tr_5=root_mean_squared_error(y_train, search5.predict(X_train.iloc[:,:58]))\n",
    "    y_te_5= search5.predict(X_test.iloc[:,:58]) \n",
    "    rmse_te_6=root_mean_squared_error(y_test, search6.predict(X_test.iloc[:,:78]))\n",
    "    rmse_tr_6=root_mean_squared_error(y_train, search6.predict(X_train.iloc[:,:78]))\n",
    "    y_te_6= search6.predict(X_test.iloc[:,:78])\n",
    "    \n",
    "    rmse_te_7=root_mean_squared_error(y_test, search7.predict(X_test.iloc[:,:40]))\n",
    "    rmse_tr_7=root_mean_squared_error(y_train, search7.predict(X_train.iloc[:,:40]))\n",
    "    y_te_7= search7.predict(X_test.iloc[:,:40])\n",
    "    rmse_te_8=root_mean_squared_error(y_test, search8.predict(X_test.iloc[:,:58]))\n",
    "    rmse_tr_8=root_mean_squared_error(y_train, search8.predict(X_train.iloc[:,:58]))\n",
    "    y_te_8= search8.predict(X_test.iloc[:,:58]) \n",
    "    rmse_te_9=root_mean_squared_error(y_test, search9.predict(X_test.iloc[:,:78]))\n",
    "    rmse_tr_9=root_mean_squared_error(y_train, search9.predict(X_train.iloc[:,:78]))\n",
    "    y_te_9= search9.predict(X_test.iloc[:,:78])\n",
    "    \n",
    "    rmse_te_10=root_mean_squared_error(y_test, search10.predict(X_test.iloc[:,:40]))\n",
    "    rmse_tr_10=root_mean_squared_error(y_train, search10.predict(X_train.iloc[:,:40]))\n",
    "    y_te_10= search10.predict(X_test.iloc[:,:40])\n",
    "    rmse_te_11=root_mean_squared_error(y_test, search11.predict(X_test.iloc[:,:58]))\n",
    "    rmse_tr_11=root_mean_squared_error(y_train, search11.predict(X_train.iloc[:,:58]))\n",
    "    y_te_11= search11.predict(X_test.iloc[:,:58]) \n",
    "    rmse_te_12=root_mean_squared_error(y_test, search12.predict(X_test.iloc[:,:78]))\n",
    "    rmse_tr_12=root_mean_squared_error(y_train, search12.predict(X_train.iloc[:,:78]))\n",
    "    y_te_12= search12.predict(X_test.iloc[:,:78])\n",
    "    \n",
    "            \n",
    "    rmse_te_13=root_mean_squared_error(y_test, search13.predict(X_test.iloc[:,:40]))\n",
    "    rmse_tr_13=root_mean_squared_error(y_train, search13.predict(X_train.iloc[:,:40]))\n",
    "    y_te_13= search13.predict(X_test.iloc[:,:40])\n",
    "    rmse_te_14=root_mean_squared_error(y_test, search14.predict(X_test.iloc[:,:58]))\n",
    "    rmse_tr_14=root_mean_squared_error(y_train, search14.predict(X_train.iloc[:,:58]))\n",
    "    y_te_14= search14.predict(X_test.iloc[:,:58]) \n",
    "    rmse_te_15=root_mean_squared_error(y_test, search15.predict(X_test.iloc[:,:78]))\n",
    "    rmse_tr_15=root_mean_squared_error(y_train, search15.predict(X_train.iloc[:,:78]))\n",
    "    y_te_15= search15.predict(X_test.iloc[:,:78])\n",
    "    \n",
    "    li1=list(y_te_1)\n",
    "    li2=list(y_te_2)\n",
    "    li3=list(y_te_3)\n",
    "    li4=list(y_te_4)\n",
    "    li5=list(y_te_5)\n",
    "    li6=list(y_te_6)\n",
    "    li7=list(y_te_7)\n",
    "    li8=list(y_te_8)\n",
    "    li9=list(y_te_9)\n",
    "    li10=list(y_te_10)\n",
    "    li11=list(y_te_11)\n",
    "    li12=list(y_te_12)\n",
    "    li13=list(y_te_13)\n",
    "    li14=list(y_te_14)\n",
    "    li15=list(y_te_15)\n",
    "    \n",
    "    \n",
    "    li1.append(rmse_te_1)\n",
    "    li1.append(rmse_tr_1)\n",
    "    li2.append(rmse_te_2)\n",
    "    li2.append(rmse_tr_2)\n",
    "    li3.append(rmse_te_3)\n",
    "    li3.append(rmse_tr_3)\n",
    "    li4.append(rmse_te_4)\n",
    "    li4.append(rmse_tr_4)\n",
    "    li5.append(rmse_te_5)\n",
    "    li5.append(rmse_tr_5)\n",
    "    li6.append(rmse_te_6)\n",
    "    li6.append(rmse_tr_6)\n",
    "    li7.append(rmse_te_7)\n",
    "    li7.append(rmse_tr_7)\n",
    "    li8.append(rmse_te_8)\n",
    "    li8.append(rmse_tr_8)\n",
    "    li9.append(rmse_te_9)\n",
    "    li9.append(rmse_tr_9)\n",
    "    li10.append(rmse_te_10)\n",
    "    li10.append(rmse_tr_10)\n",
    "    li11.append(rmse_te_11)\n",
    "    li11.append(rmse_tr_11)\n",
    "    li12.append(rmse_te_12)\n",
    "    li12.append(rmse_tr_12)\n",
    "    li13.append(rmse_te_13)\n",
    "    li13.append(rmse_tr_13)\n",
    "    li14.append(rmse_te_14)\n",
    "    li14.append(rmse_tr_14)\n",
    "    li15.append(rmse_te_15)\n",
    "    li15.append(rmse_tr_15)\n",
    "    \n",
    "\n",
    "    l1.append(li1)\n",
    "    l2.append(li2)\n",
    "    l3.append(li3)\n",
    "    l4.append(li4)\n",
    "    l5.append(li5)\n",
    "    l6.append(li6)    \n",
    "    l7.append(li7)\n",
    "    l8.append(li8)\n",
    "    l9.append(li9)\n",
    "    l10.append(li10)\n",
    "    l11.append(li11)\n",
    "    l12.append(li12)    \n",
    "    l13.append(li13)\n",
    "    l14.append(li14)\n",
    "    l15.append(li15)\n",
    "\n",
    "\n",
    "l1.insert(0,list(y_test))\n",
    "data=pd.concat([pd.DataFrame(l1),pd.DataFrame(l2),pd.DataFrame(l3),pd.DataFrame(l4),pd.DataFrame(l5),pd.DataFrame(l6),pd.DataFrame(l7),pd.DataFrame(l8),pd.DataFrame(l9),pd.DataFrame(l10),pd.DataFrame(l11),pd.DataFrame(l12),pd.DataFrame(l13),pd.DataFrame(l14),pd.DataFrame(l15)],axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>700.000000</td>\n",
       "      <td>700.000000</td>\n",
       "      <td>700.000000</td>\n",
       "      <td>900.000000</td>\n",
       "      <td>900.000000</td>\n",
       "      <td>900.000000</td>\n",
       "      <td>900.000000</td>\n",
       "      <td>900.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-23.995726</td>\n",
       "      <td>-16.356190</td>\n",
       "      <td>0.424127</td>\n",
       "      <td>14.546635</td>\n",
       "      <td>1.736667</td>\n",
       "      <td>-1.861955</td>\n",
       "      <td>8.929655</td>\n",
       "      <td>9.227757</td>\n",
       "      <td>2.409505</td>\n",
       "      <td>2.922653</td>\n",
       "      <td>...</td>\n",
       "      <td>692.360698</td>\n",
       "      <td>702.116371</td>\n",
       "      <td>660.382942</td>\n",
       "      <td>974.261690</td>\n",
       "      <td>905.927464</td>\n",
       "      <td>846.441950</td>\n",
       "      <td>993.781070</td>\n",
       "      <td>866.780468</td>\n",
       "      <td>29.332574</td>\n",
       "      <td>27.571854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.697523</td>\n",
       "      <td>-7.323848</td>\n",
       "      <td>-2.667502</td>\n",
       "      <td>-1.148596</td>\n",
       "      <td>-2.398201</td>\n",
       "      <td>4.838327</td>\n",
       "      <td>4.786494</td>\n",
       "      <td>2.411714</td>\n",
       "      <td>0.897033</td>\n",
       "      <td>9.774132</td>\n",
       "      <td>...</td>\n",
       "      <td>678.258741</td>\n",
       "      <td>663.911730</td>\n",
       "      <td>739.062981</td>\n",
       "      <td>796.968880</td>\n",
       "      <td>826.045116</td>\n",
       "      <td>868.543437</td>\n",
       "      <td>871.339053</td>\n",
       "      <td>882.937999</td>\n",
       "      <td>24.133809</td>\n",
       "      <td>28.672825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.060673</td>\n",
       "      <td>11.877368</td>\n",
       "      <td>-12.477928</td>\n",
       "      <td>-11.135747</td>\n",
       "      <td>5.816383</td>\n",
       "      <td>18.592508</td>\n",
       "      <td>-10.095121</td>\n",
       "      <td>15.834216</td>\n",
       "      <td>19.174302</td>\n",
       "      <td>4.577096</td>\n",
       "      <td>...</td>\n",
       "      <td>662.957564</td>\n",
       "      <td>741.814975</td>\n",
       "      <td>725.095365</td>\n",
       "      <td>803.308742</td>\n",
       "      <td>836.465803</td>\n",
       "      <td>882.896345</td>\n",
       "      <td>823.645035</td>\n",
       "      <td>959.315153</td>\n",
       "      <td>26.628438</td>\n",
       "      <td>28.243385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-4.970896</td>\n",
       "      <td>-2.482750</td>\n",
       "      <td>-1.494768</td>\n",
       "      <td>-12.161511</td>\n",
       "      <td>-2.621207</td>\n",
       "      <td>3.155111</td>\n",
       "      <td>1.912214</td>\n",
       "      <td>6.411200</td>\n",
       "      <td>19.245301</td>\n",
       "      <td>3.697501</td>\n",
       "      <td>...</td>\n",
       "      <td>681.054798</td>\n",
       "      <td>733.716376</td>\n",
       "      <td>684.659903</td>\n",
       "      <td>900.580880</td>\n",
       "      <td>856.008488</td>\n",
       "      <td>856.727441</td>\n",
       "      <td>831.956516</td>\n",
       "      <td>904.008243</td>\n",
       "      <td>22.584470</td>\n",
       "      <td>29.037250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.800162</td>\n",
       "      <td>5.921838</td>\n",
       "      <td>7.297583</td>\n",
       "      <td>5.980520</td>\n",
       "      <td>7.569100</td>\n",
       "      <td>7.241476</td>\n",
       "      <td>11.431052</td>\n",
       "      <td>10.397264</td>\n",
       "      <td>12.293756</td>\n",
       "      <td>9.526074</td>\n",
       "      <td>...</td>\n",
       "      <td>682.045166</td>\n",
       "      <td>687.625793</td>\n",
       "      <td>680.615967</td>\n",
       "      <td>893.810974</td>\n",
       "      <td>891.717163</td>\n",
       "      <td>902.936401</td>\n",
       "      <td>921.967163</td>\n",
       "      <td>897.711548</td>\n",
       "      <td>30.337802</td>\n",
       "      <td>0.009989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.951788</td>\n",
       "      <td>7.385530</td>\n",
       "      <td>5.596478</td>\n",
       "      <td>6.822535</td>\n",
       "      <td>7.167724</td>\n",
       "      <td>10.691855</td>\n",
       "      <td>5.968075</td>\n",
       "      <td>9.216184</td>\n",
       "      <td>9.684932</td>\n",
       "      <td>7.046870</td>\n",
       "      <td>...</td>\n",
       "      <td>705.281067</td>\n",
       "      <td>659.174072</td>\n",
       "      <td>705.564331</td>\n",
       "      <td>874.654602</td>\n",
       "      <td>891.149536</td>\n",
       "      <td>898.414673</td>\n",
       "      <td>907.783936</td>\n",
       "      <td>901.424744</td>\n",
       "      <td>13.073535</td>\n",
       "      <td>0.004993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.667495</td>\n",
       "      <td>6.911593</td>\n",
       "      <td>6.178175</td>\n",
       "      <td>5.954823</td>\n",
       "      <td>9.910942</td>\n",
       "      <td>13.181696</td>\n",
       "      <td>7.731301</td>\n",
       "      <td>15.950731</td>\n",
       "      <td>25.196522</td>\n",
       "      <td>13.904265</td>\n",
       "      <td>...</td>\n",
       "      <td>672.388000</td>\n",
       "      <td>715.168762</td>\n",
       "      <td>817.438477</td>\n",
       "      <td>882.030457</td>\n",
       "      <td>895.306641</td>\n",
       "      <td>919.489441</td>\n",
       "      <td>894.505798</td>\n",
       "      <td>911.085876</td>\n",
       "      <td>21.610715</td>\n",
       "      <td>0.003968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.515472</td>\n",
       "      <td>7.573937</td>\n",
       "      <td>6.990186</td>\n",
       "      <td>8.785959</td>\n",
       "      <td>9.843958</td>\n",
       "      <td>11.364626</td>\n",
       "      <td>10.308897</td>\n",
       "      <td>7.930188</td>\n",
       "      <td>13.574883</td>\n",
       "      <td>11.211787</td>\n",
       "      <td>...</td>\n",
       "      <td>708.233704</td>\n",
       "      <td>821.670654</td>\n",
       "      <td>699.797119</td>\n",
       "      <td>918.994324</td>\n",
       "      <td>881.358948</td>\n",
       "      <td>894.479980</td>\n",
       "      <td>885.717529</td>\n",
       "      <td>891.269775</td>\n",
       "      <td>24.702674</td>\n",
       "      <td>0.013100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.019098</td>\n",
       "      <td>8.318549</td>\n",
       "      <td>6.038330</td>\n",
       "      <td>5.979350</td>\n",
       "      <td>6.935136</td>\n",
       "      <td>8.630899</td>\n",
       "      <td>8.698400</td>\n",
       "      <td>9.375732</td>\n",
       "      <td>11.379828</td>\n",
       "      <td>14.595894</td>\n",
       "      <td>...</td>\n",
       "      <td>687.378845</td>\n",
       "      <td>744.804749</td>\n",
       "      <td>814.273499</td>\n",
       "      <td>904.140503</td>\n",
       "      <td>829.147095</td>\n",
       "      <td>898.568115</td>\n",
       "      <td>914.580750</td>\n",
       "      <td>910.313660</td>\n",
       "      <td>23.520435</td>\n",
       "      <td>0.000830</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76 rows Ã 72 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0          1          2          3         4          5   \\\n",
       "0    5.000000   5.000000   5.000000   5.000000  5.000000  10.000000   \n",
       "1  -23.995726 -16.356190   0.424127  14.546635  1.736667  -1.861955   \n",
       "2   12.697523  -7.323848  -2.667502  -1.148596 -2.398201   4.838327   \n",
       "3    2.060673  11.877368 -12.477928 -11.135747  5.816383  18.592508   \n",
       "4   -4.970896  -2.482750  -1.494768 -12.161511 -2.621207   3.155111   \n",
       "..        ...        ...        ...        ...       ...        ...   \n",
       "0    7.800162   5.921838   7.297583   5.980520  7.569100   7.241476   \n",
       "1    6.951788   7.385530   5.596478   6.822535  7.167724  10.691855   \n",
       "2    9.667495   6.911593   6.178175   5.954823  9.910942  13.181696   \n",
       "3    3.515472   7.573937   6.990186   8.785959  9.843958  11.364626   \n",
       "4    4.019098   8.318549   6.038330   5.979350  6.935136   8.630899   \n",
       "\n",
       "           6          7          8          9   ...          62          63  \\\n",
       "0   10.000000  10.000000  10.000000  10.000000  ...  700.000000  700.000000   \n",
       "1    8.929655   9.227757   2.409505   2.922653  ...  692.360698  702.116371   \n",
       "2    4.786494   2.411714   0.897033   9.774132  ...  678.258741  663.911730   \n",
       "3  -10.095121  15.834216  19.174302   4.577096  ...  662.957564  741.814975   \n",
       "4    1.912214   6.411200  19.245301   3.697501  ...  681.054798  733.716376   \n",
       "..        ...        ...        ...        ...  ...         ...         ...   \n",
       "0   11.431052  10.397264  12.293756   9.526074  ...  682.045166  687.625793   \n",
       "1    5.968075   9.216184   9.684932   7.046870  ...  705.281067  659.174072   \n",
       "2    7.731301  15.950731  25.196522  13.904265  ...  672.388000  715.168762   \n",
       "3   10.308897   7.930188  13.574883  11.211787  ...  708.233704  821.670654   \n",
       "4    8.698400   9.375732  11.379828  14.595894  ...  687.378845  744.804749   \n",
       "\n",
       "            64          65          66          67          68          69  \\\n",
       "0   700.000000  900.000000  900.000000  900.000000  900.000000  900.000000   \n",
       "1   660.382942  974.261690  905.927464  846.441950  993.781070  866.780468   \n",
       "2   739.062981  796.968880  826.045116  868.543437  871.339053  882.937999   \n",
       "3   725.095365  803.308742  836.465803  882.896345  823.645035  959.315153   \n",
       "4   684.659903  900.580880  856.008488  856.727441  831.956516  904.008243   \n",
       "..         ...         ...         ...         ...         ...         ...   \n",
       "0   680.615967  893.810974  891.717163  902.936401  921.967163  897.711548   \n",
       "1   705.564331  874.654602  891.149536  898.414673  907.783936  901.424744   \n",
       "2   817.438477  882.030457  895.306641  919.489441  894.505798  911.085876   \n",
       "3   699.797119  918.994324  881.358948  894.479980  885.717529  891.269775   \n",
       "4   814.273499  904.140503  829.147095  898.568115  914.580750  910.313660   \n",
       "\n",
       "           70         71  \n",
       "0         NaN        NaN  \n",
       "1   29.332574  27.571854  \n",
       "2   24.133809  28.672825  \n",
       "3   26.628438  28.243385  \n",
       "4   22.584470  29.037250  \n",
       "..        ...        ...  \n",
       "0   30.337802   0.009989  \n",
       "1   13.073535   0.004993  \n",
       "2   21.610715   0.003968  \n",
       "3   24.702674   0.013100  \n",
       "4   23.520435   0.000830  \n",
       "\n",
       "[76 rows x 72 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
